{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PS6 - CE264\n",
    "# GSI: Mustapha Harb, Mengqiao Yu, Andrew Campbell\n",
    "\n",
    "# importing the requried libraries\n",
    "from collections import OrderedDict    # For recording the model specification \n",
    "\n",
    "import pandas as pd                    # For file input/output\n",
    "import numpy as np                     # For vectorized math operations\n",
    "\n",
    "import pylogit as pl                   # For MNL model estimation and\n",
    "                                       # conversion from wide to long format\n",
    "    \n",
    "%matplotlib inline\n",
    "\n",
    "# reading the data file \n",
    "data_wide  = pd.read_csv(\"data01.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: MNL Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MNL specifications to predict mode choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting the data from wide to long format\n",
    "\n",
    "# Create the list of individual specific variables\n",
    "ind_variables = data_wide.columns.tolist()[:2] + [\"weights\"]\n",
    "\n",
    "# Specify the variables that vary across individuals and some or all alternatives\n",
    "# The keys are the column names that will be used in the long format dataframe.\n",
    "# The values are dictionaries whose key-value pairs are the alternative id and\n",
    "# the column name of the corresponding column that encodes that variable for\n",
    "# the given alternative. Examples below.\n",
    "alt_varying_variables = {u'travel_time': dict([(1, 'tt_da'),\n",
    "                                               (2, 'tt_sr'),\n",
    "                                               (3, 'tt_walk'),\n",
    "                                               (4, 'tt_bike'),\n",
    "                                               (5, 'tt_wt'),\n",
    "                                               (6, 'tt_dt')]),\n",
    "                          u'distance_car': dict([(1, 'dist_car'),\n",
    "                                                (2, 'dist_car')]),\n",
    "                          u'travel_cost': dict([(1, 'cost_da'),\n",
    "                                                (2, 'cost_sr'),\n",
    "                                                (5, 'cost_wt'),\n",
    "                                                (6, 'cost_dt')]),\n",
    "                          u'access_time': dict([(5, 'accTime_wt'),\n",
    "                                                (6, 'accTime_dt')]),\n",
    "                          u'egress_time': dict([(5, 'egrTime_wt'),\n",
    "                                                (6, 'egrTime_dt')]),\n",
    "                          u'initial_wait': dict([(5, 'iWait_wt'),\n",
    "                                                 (6, 'iWait_dt')]),\n",
    "                          u'transfer_wait': dict([(5, 'xWait_wt'),\n",
    "                                                  (6, 'xWait_dt')]),\n",
    "                          u'access_distance_dt': dict([(6, \"accDist_dt\")])}\n",
    "\n",
    "# Specify the availability variables\n",
    "# Note that the keys of the dictionary are the alternative id's.\n",
    "# The values are the columns denoting the availability for the\n",
    "# given mode in the dataset.\n",
    "\n",
    "\n",
    "availability_variables = {1: 'avail_da',\n",
    "                          2: 'avail_sr', \n",
    "                          3: 'avail_walk',\n",
    "                          4: 'avail_bike',\n",
    "                          5: 'avail_wt',\n",
    "                          6: 'avail_dt'}\n",
    "\n",
    "##########\n",
    "# Determine the columns for: alternative ids, the observation ids and the choice\n",
    "##########\n",
    "# The 'custom_alt_id' is the name of a column to be created in the long-format data\n",
    "# It will identify the alternative associated with each row.\n",
    "custom_alt_id = \"mode_id\"\n",
    "\n",
    "# Create a custom id column that ignores the fact that this is a \n",
    "# panel/repeated-observations dataset. Note the +1 ensures the id's start at one.\n",
    "obs_id_column = \"obsID\"\n",
    "\n",
    "# Create a variable recording the choice column\n",
    "choice_column = \"choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the conversion to long-format\n",
    "data_long = pl.convert_wide_to_long(data_wide, \n",
    "                                           ind_variables, \n",
    "                                           alt_varying_variables, \n",
    "                                           availability_variables, \n",
    "                                           obs_id_column, \n",
    "                                           choice_column,\n",
    "                                           new_alt_id_name=custom_alt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Create scaled variables so the estimated coefficients are of similar magnitudes\n",
    "##########\n",
    "# Scale the travel time column by 60 to convert raw units (minutes) to hours\n",
    "data_long[\"travel_time_hrs\"] = data_long[\"travel_time\"] / 60.0\n",
    "\n",
    "# Scale the access by 60 to convert raw units (minutes) to hours\n",
    "data_long[\"access_time_hrs\"] = data_long[\"access_time\"] / 60.0\n",
    "\n",
    "# for drive to transit let us combine travel time and access time\n",
    "data_long[\"travel_time_access_time_hrs\"] = data_long[\"travel_time_hrs\"] + data_long[\"access_time_hrs\"]\n",
    "\n",
    "#Scale the egress time by 60\n",
    "data_long[\"egress_time_hrs\"] = data_long[\"egress_time\"] / 60.0\n",
    "\n",
    "# combining access and egress time which we want to use for the walk to transit alternative\n",
    "data_long[\"acess_egress_hrs\"] = data_long[\"access_time_hrs\"] + data_long[\"egress_time_hrs\"]\n",
    "\n",
    "# scaling the initial wait by 60\n",
    "data_long[\"initial_wait_hrs\"] = data_long[\"initial_wait\"] / 60.0\n",
    "\n",
    "# scaling the transfer wait by 60\n",
    "data_long[\"transfer_wait_hrs\"]  = data_long[\"transfer_wait\"] / 60.0\n",
    "\n",
    "# combining transfer wait and initial wait to be used for walk to transit and bike to transit\n",
    "data_long[\"initial_transfer_wait_hrs\"] = data_long[\"initial_wait_hrs\"] + data_long[\"transfer_wait_hrs\"]\n",
    "\n",
    "\n",
    "# creating non-linear transformations for the cost variable\n",
    "cutOff1 = 2\n",
    "cutOff2 = 7\n",
    "\n",
    "\n",
    "data_long[\"cost_cat_one\"] = (data_long[\"travel_cost\"] <= cutOff1)*data_long[\"travel_cost\"] + (data_long[\"travel_cost\"] > cutOff1)*cutOff1\n",
    "\n",
    "data_long[\"cost_cat_two\"] = (data_long[\"travel_cost\"] > cutOff1)*(data_long[\"travel_cost\"] <= cutOff2)*(data_long[\"travel_cost\"] - cutOff1) + (data_long[\"travel_cost\"] > cutOff2)* (cutOff2 - cutOff1)\n",
    "\n",
    "data_long[\"cost_cat_three\"] = (data_long[\"travel_cost\"] > cutOff2)*(data_long[\"travel_cost\"] - cutOff2)\n",
    "\n",
    "\n",
    "\n",
    "# specifying the utility equations\n",
    "\n",
    "# NOTE: - Specification and variable names must be ordered dictionaries.\n",
    "#       - Keys should be variables within the long format dataframe.\n",
    "#         The sole exception to this is the \"intercept\" key.\n",
    "#       - For the specification dictionary, the values should be lists\n",
    "#         of integers or or lists of lists of integers. Within a list, \n",
    "#         or within the inner-most list, the integers should be the \n",
    "#         alternative ID's of the alternative whose utility specification \n",
    "#         the explanatory variable is entering. Lists of lists denote \n",
    "#         alternatives that will share a common coefficient for the variable\n",
    "#         in question.\n",
    "\n",
    "basic_specification = OrderedDict()\n",
    "basic_names = OrderedDict()\n",
    "\n",
    "\n",
    "basic_specification[\"intercept\"] = [ 2, 3, 4, 5, 6]\n",
    "basic_names[\"intercept\"] = ['ASC SR',\n",
    "                            'ASC Walk', 'ASC Bike', 'ASC WT', 'ASC DT']\n",
    "\n",
    "basic_specification[\"travel_time_hrs\"] = [[1, 2, 5], 4, 3]\n",
    "basic_names[\"travel_time_hrs\"] = ['In-Vehicle Travel Time, units:hrs (DA, SR, WT)',\n",
    "                                  'Bike Time, units:hrs (Bike)',\n",
    "                                  'Walk Time, units:hrs (Walk)']\n",
    "\n",
    "basic_specification[\"travel_time_access_time_hrs\"] = [6]\n",
    "basic_names[\"travel_time_access_time_hrs\"] = [\"In-Vehicle Travel Time, units:hrs, (DT)\"]\n",
    "\n",
    "basic_specification[\"acess_egress_hrs\"] = [5]\n",
    "basic_names[\"acess_egress_hrs\"] = [\"Walk Time, units:hrs, (WT)\"]\n",
    "\n",
    "basic_specification[\"egress_time_hrs\"] = [6]\n",
    "basic_names[\"egress_time_hrs\"] = [\"Walk Time, units:hrs, (DT)\"]\n",
    "\n",
    "basic_specification[\"initial_transfer_wait_hrs\"] = [[5, 6]]\n",
    "basic_names[\"initial_transfer_wait_hrs\"] = [\"Waiting Time, units:hrs, (WT and DT)\"]\n",
    "\n",
    "\n",
    "basic_specification[\"cost_cat_one\"] = [[1, 2, 5,6]]\n",
    "basic_names[\"cost_cat_one\"] = ['Cost: Under $2']\n",
    "\n",
    "basic_specification[\"cost_cat_two\"] = [[1, 2, 5,6]]\n",
    "basic_names[\"cost_cat_two\"] = ['Cost: (2 - 7)$']\n",
    "\n",
    "basic_specification[\"cost_cat_three\"] = [[1, 2, 5,6]]\n",
    "basic_names[\"cost_cat_three\"] = ['Cost: Above $7']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a - Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### taking a sample of 10,000 observation from the BATS 2000 dataset \n",
    "# and split it into train and test sets\n",
    "train_size = 8000\n",
    "total_size = 10000\n",
    "\n",
    "# Suffle the data before sampling\n",
    "np.random.seed(1138)  \n",
    "shuffled = np.random.choice(data_long.obsID.unique()[0:total_size], total_size, replace=False)\n",
    "train_ids = shuffled[0:train_size]\n",
    "test_ids = shuffled[train_size:]\n",
    "\n",
    "new_data_train = data_long.loc[data_long[obs_id_column].isin(train_ids)].copy()\n",
    "new_data_test = data_long.loc[data_long[obs_id_column].isin(test_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_ids.size\n",
    "print test_ids.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model and inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate the multinomial logit model (MNL)\n",
    "data_mnl = pl.create_choice_model(data=new_data_train,\n",
    "                                  alt_id_col=custom_alt_id,\n",
    "                                  obs_id_col=obs_id_column,\n",
    "                                  choice_col=choice_column,\n",
    "                                  specification=basic_specification,\n",
    "                                  model_type=\"MNL\",\n",
    "                                  names=basic_names)\n",
    "\n",
    "# Specify the initial values and method for the optimization.\n",
    "data_mnl.fit_mle(np.zeros(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the estimation results\n",
    "data_mnl.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b - MNL Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model estimated on the *training data* to predict the probability of each mode for the *test data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_array_test = data_mnl.predict(new_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add the predicted probabilities to the test dataset\n",
    "new_data_test['predict'] = prediction_array_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the accuracy of our MNL model for the *test data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_test = new_data_test.groupby(u'obsID').apply(lambda x: np.argmax(x.predict))  # indices of predicted choices in the long data\n",
    "predicted_actual_test = new_data_test.loc[idx_test, :]['choice']  # actual outcomes for the predicted choices                                                                                                              # [0 = pred wrong, 1 = pred right]\n",
    "accuracy_test = np.true_divide(np.sum(predicted_actual_test), predicted_actual_test.size)\n",
    "print accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> \n",
    "### The accuracy of the MNL model is 0.696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model across a range of training sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot comparing training and test accuracy for differenet training sample size. Keep the total_size fixed at 10,000, but range across the train_size. Make sure you clude some small training sample sizes. I recommend using some different intervals like: [100, 200, 500, 1000, 2000, 3000, ... 9000]\n",
    "\n",
    "Use the above code as template. We recommend using a for loop to iterate through all the train_size values instead of doing it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions should simplify things for your for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return shuffled training and data sets\n",
    "def split_data(data, train_size, test_size, seed=1138):\n",
    "    \"\"\"\n",
    "    Takes a long-form DataFrame and returns mutually exclusive training and test samples of specificed lengths. Input\n",
    "    data observation ids are shuffled to ensure random draws for samples.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- long format DataFrame\n",
    "    train_size -- integer size of training sample\n",
    "    test_size -- integer size of test sample\n",
    "    seed -- integer to set random seed for Numpy\n",
    "    \"\"\"\n",
    "    # Suffle the data before sampling\n",
    "    np.random.seed(seed)  \n",
    "    shuffled = np.random.choice(data.obsID.unique()[0:train_size+test_size], train_size+test_size, replace=False)\n",
    "    train_ids = shuffled[0:train_size]\n",
    "    test_ids = shuffled[train_size:]\n",
    "\n",
    "    train = data.loc[data[obs_id_column].isin(train_ids)].copy()\n",
    "    test = data.loc[data[obs_id_column].isin(test_ids)].copy()\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy and log-likelihood using a trained MNL and data\n",
    "def eval_prediction(model, data):\n",
    "    \"\"\"\n",
    "    Function to calculate to evaluate model estimates. It returns a tuple of 1) accuracy, 2) log-likelihood\n",
    "    3) null log-likelihood  4) likelihood ratio index and 5) adjusted likelihood ratio index\n",
    "    \n",
    "    Keyword arguments:\n",
    "    model -- a trained PyLogit model\n",
    "    data -- (N x K) Pandas DataFrame long format. Same attributes as that used to train the model\n",
    "    \n",
    "    returns -- Tuple: (accuracy, log-likelihood, null log-likelihood, likelihood ratio index,\n",
    "    adjusted likelihood ratio index)\n",
    "    \"\"\"\n",
    "    ##\n",
    "    # Calculate the accuracy\n",
    "    ##\n",
    "    \n",
    "    # Predicted choice probabilities \n",
    "    prediction_array = model.predict(data)\n",
    "    data['predict'] = prediction_array\n",
    "    idx = data.groupby(u'obsID').apply(lambda x: np.argmax(x.predict))  # indices of predicted choices in the long data\n",
    "    predicted_actual = data.loc[idx, :]['choice']  # actual outcomes for the predicted choices\n",
    "    accuracy = np.true_divide(np.sum(predicted_actual), predicted_actual.size)\n",
    "    \n",
    "    ##\n",
    "    # Calculate the log-likelihood\n",
    "    ##\n",
    "    ll = np.dot(data.choice, np.log(prediction_array))\n",
    "    \n",
    "    ##\n",
    "    # Calculate the likelihood ratio index\n",
    "    ##\n",
    "\n",
    "    # calc the null predictions\n",
    "    null_prediction_array = data.groupby(u'obsID')[u'obsID'].transform(lambda x: np.true_divide(1, x.shape[0]))\n",
    "    ll_0 = np.dot(data.choice, np.log(null_prediction_array))\n",
    "    rho_2 = 1 - np.abs(np.true_divide(ll,ll_0))\n",
    "    \n",
    "    ##\n",
    "    # Calc adjusted likelihood ratio index\n",
    "    ##\n",
    "    rho_bar_2 = 1 - np.abs(np.true_divide(ll - model.params.shape[0], ll_0))\n",
    "    \n",
    "    return (accuracy, ll, ll_0, rho_2, rho_bar_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and test log-likelihoods of the model for the same training sizes used above. Either add a second y-axis to the accuracy plot, or make a new plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Use ML algorithms we learned in class to predict mode choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "### The next 3 cells are just to fix the data to match the dataset used to train the MNL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data_wide\n",
    "df = df[:9999]\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mins_to_hours_cols = ['tt_da', 'tt_sr', 'tt_walk', 'tt_bike', \n",
    "                     'tt_wt', 'accTime_wt', 'egrTime_wt',\n",
    "                     'iWait_wt', 'xWait_wt', 'tt_dt', \n",
    "                     'accTime_dt','egrTime_dt', 'iWait_dt', \n",
    "                     'xWait_dt']\n",
    "\n",
    "for col in mins_to_hours_cols:\n",
    "    df[col+\"_hrs\"] = df[col]/60\n",
    "\n",
    "df[\"travel_time_access_time_hrs_dt\"] = df[\"tt_dt_hrs\"] + df[\"accTime_dt_hrs\"]\n",
    "df[\"travel_time_access_time_hrs_wt\"] = df[\"tt_wt_hrs\"] + df[\"accTime_wt_hrs\"]\n",
    "\n",
    "df[\"access_egress_hrs_dt\"] = df[\"accTime_dt_hrs\"] + df[\"egrTime_dt_hrs\"]\n",
    "df[\"access_egress_hrs_wt\"] = df[\"accTime_wt_hrs\"] + df[\"egrTime_wt_hrs\"]\n",
    "\n",
    "df[\"initial_transfer_wait_hrs_dt\"] = df[\"iWait_dt_hrs\"] + df[\"xWait_dt_hrs\"]\n",
    "df[\"initial_transfer_wait_hrs_wt\"] = df[\"iWait_wt_hrs\"] + df[\"xWait_wt_hrs\"]\n",
    "\n",
    "# creating non-linear transformations for the cost variable\n",
    "cutOff1 = 2\n",
    "cutOff2 = 7\n",
    "\n",
    "df[\"cost_cat_one_da\"] = (df['cost_da']<=cutOff1)*df['cost_da'] + (df['cost_da'] > cutOff1)*cutOff1\n",
    "df[\"cost_cat_two_da\"] = (df['cost_da'] > cutOff1)*(df['cost_da'] <= cutOff2)*(df['cost_da'] - cutOff1) + (df['cost_da'] > cutOff2)* (cutOff2 - cutOff1)\n",
    "df[\"cost_cat_three_da\"] = (df['cost_da'] > cutOff2)*(df['cost_da'] - cutOff2)\n",
    "\n",
    "df[\"cost_cat_one_sr\"] = (df['cost_sr']<=cutOff1)*df['cost_sr'] + (df['cost_sr'] > cutOff1)*cutOff1\n",
    "df[\"cost_cat_two_sr\"] = (df['cost_sr'] > cutOff1)*(df['cost_sr'] <= cutOff2)*(df['cost_sr'] - cutOff1) + (df['cost_sr'] > cutOff2)* (cutOff2 - cutOff1)\n",
    "df[\"cost_cat_three_sr\"] = (df['cost_sr'] > cutOff2)*(df['cost_sr'] - cutOff2)\n",
    "\n",
    "df[\"cost_cat_one_dt\"] = (df['cost_dt']<=cutOff1)*df['cost_dt'] + (df['cost_dt'] > cutOff1)*cutOff1\n",
    "df[\"cost_cat_two_dt\"] = (df['cost_dt'] > cutOff1)*(df['cost_dt'] <= cutOff2)*(df['cost_dt'] - cutOff1) + (df['cost_dt'] > cutOff2)* (cutOff2 - cutOff1)\n",
    "df[\"cost_cat_three_dt\"] = (df['cost_dt'] > cutOff2)*(df['cost_dt'] - cutOff2)\n",
    "\n",
    "df[\"cost_cat_one_wt\"] = (df['cost_wt']<=cutOff1)*df['cost_wt'] + (df['cost_wt'] > cutOff1)*cutOff1\n",
    "df[\"cost_cat_two_wt\"] = (df['cost_wt'] > cutOff1)*(df['cost_wt'] <= cutOff2)*(df['cost_wt'] - cutOff1) + (df['cost_wt'] > cutOff2)* (cutOff2 - cutOff1)\n",
    "df[\"cost_cat_three_wt\"] = (df['cost_wt'] > cutOff2)*(df['cost_wt'] - cutOff2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cols = pd.DataFrame(df.columns,columns=[\"col_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(df.columns.values)\n",
    "cols_to_include = (cols[45:] + [\"egrTime_wt_hrs\",\"egrTime_dt_hrs\"]+\n",
    "                   cols[23:29] + cols[31:36] + [\"tt_dt_hrs\"] + [\"choice\"])\n",
    "\n",
    "new_df = df[cols_to_include]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use sklearn to split the data into a training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X = new_df.drop([\"choice\"],axis=1)\n",
    "y = new_df[\"choice\"]\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code:Training and testing a KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1: import the sklarn package that allows you to do KNN in python\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2: set up the name of your classifier and the hyper-parameters you want to use\n",
    "KNN_classifier = neighbors.KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 3: train/fit the classifier on training data\n",
    "KNN_classifier.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 4: use your clasifier to predict mode choice for test data\n",
    "KNN_predictions = KNN_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 5: compare your predictions with the actual choices\n",
    "KNN_predictions == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 6: calculate accuracy (also called score)\n",
    "sum(KNN_predictions == y_test)/float(len(KNN_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 7: repeat all steps for a different hyper parameter (in this case number of neighbers K). \n",
    "<font color=red> \n",
    "*best way to do so is run a for loop instead of repeating the steps and changing the parameter by hand*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Note: an easier way to do steps 4, 5 and 6 is to use the *score* function in sklearn (see example below).\n",
    "\n",
    "I highly recommend you do not do that for this assignment to learn more by doing things manually rather than running a line of code that does things for you without really understanding much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "KNN_classifier.score(X = X_test, y = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. number of neighors for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. number of neighors for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test a Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph of accuracy vs. hyper_parameters for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table for all the algorithms you used and their highest accuracy (including your MNL model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
